--- 
title: "Self-reported voter turnout"
author: "Jiyeon Chang"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---
```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

# Introduction

Average turnout in US elections hover around 50-60%, but in surveys, the figure is often closer to 70-80%. Why is that? It's possible that this gap is attributable to nonresponse, but some point out that there is an element of social desirability bias in the way people respond to survey questions. In other words, there are people who lie about whether they voted, i.e. they say they voted even though in reality they did not. This has been a tricky issue for survey researchers, as you can't take survey response at face value. In this project, I look at what are some of the variables that can may explain over-reporting pattern.

The main dataset I use to explore this question is the Cooperative Congressional Election Study (https://cces.gov.harvard.edu/). The survey asks respondents whether they voted in the recent election. For each respondent, there is a variable indicating whether there exists a record of that person of having voted. So the strategy here is to look at respondents for whom there is *no* record of voting, and compare the group who said they voted (record does not match what they say) vs. those who say that they did not vote (missing record matches their statement). For those who claim to have voted, there are two possible explanations for why there might not be a corresponding voting record for them. First, it may have been an administrative error--if their address has changed recently, or some other identifying information does not match, it's possible that the missing record is not an indication that they are lying. Alternatively, the respondent may not be telling the truth. There is not definitive way to distinguish which of the observations are due to error or due to lying. However, if we can assume that the rate of administrative error should be random across different populations, then it *is* possible to see how share of those who claim to have voted (vs. not) differ across individuals with specific demographic, political or social traits.

Specifically, I report the result of looking at the following variables:

- a) state (proxies political climate)
- b) age; income; race; political affiliation; strength of partisanship (demographic variables)
- c) how long Rs have lived at the current address; frequency of church attendance (ties to community)
- d) by election cycle; general election including presidential vs. off yr etc. (election specific variables)
- e) turnout; margin of elections (This data will come from the MIT Elections Data) (if time allows)

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Data sources

The main dataset I use is the Cooperative Congressional Election Study (https://cces.gov.harvard.edu/). CCES is a 50,000+ person national stratified sample survey administered by YouGov on a biannual basis. For the purpose of this project, I look at the 3 most recent presidential elections for which data is available--that is, 2008, 2012 and 2016. The data from 2016, for instance, includes 64,600 observations and 563 variables. However, I look a subset of this sample, namely those for whom there is no voting record. For the 3 years, this reduces the sample to a total of 13,216 observations. A wide range of variables are included in the dataset, including the main demographic variables (age, gender, state/county of residence, race, education, income, etc.) and an extensive list of questions about political affiliation and attitudes, support for specific candidates and opinions about proposed policies, such as climate change, and gun control.

As briefly mentioned above, one of the challenges of using this data is that, for those who claim to have voted (but for whom there is no matching voting record), we won't ever know whether these people are lying or if there's been an administrative error. Literature pertaining to the field identifies a number of factors that may predict a higher probability of admin error, e.g. recent change in residence. There are many variables for which the rate of administrate error should not differ across the response categories, e.g. gender, or religion. This is more a problem with the conceptualization of the question rather than a problem with the quality of data, however, and I will address the limitations in the conclusions one can draw from the visualization that I will produce.

<!--chapter:end:02-data.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Data transformation

- The data has been prepared by merging data from 3 election years, and also merging the CCES data with electoral outcome dataset to create variables accounting for state level turnout and margin of victory. It takes a while to load the data, so I display the code below but don't run it, and the data used for the next section calls it from a csv file I saved in earlier runs. The main steps I undertook involved the following:

- Rename variables from each year to prepare for the merger. A similar set of questions were asked each year but they didn't necessarily have the same variable names.
- Recode variables with several levels into binary variables where it is helpful: This is done for variables such as church01, which is 1 if the person goes to church at least several times a year and 0 otherwise. (The original variable had more detailed frequencies.) Similar steps were taken to indicate stable_residency01, which is 1 if the person has been residing at current address for >1 year.
- The electoral outcome at state-year level from the MIT Elections data was merged with the respondent's state-year to create variables indicating how big the margin was in the race they voted, and which party won. These variables make it possible to capture the "political reality" of the electoral unit (state) in which each respondent is voting. Whether this political reality predicts differential rate of overreporting corresponds to item (e) on my list of questions; as mentioned above I'm not sure if I'll have time to get to it but the variables have been merged and created!
```{r}
devtools::install_github("hrbrmstr/statebins")
library(dplyr)
library(tidyverse)
df.self<-read.csv("/Users/siyeona/cleaned_data.csv")
df.self$year<-as.factor(df.self$year)
```

<!--chapter:end:03-cleaning.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Missing values

My dataset is quite large, so for the purpose of visualization, I restrict the analysis to 2016, and assume that the pattern will be similar for years 2012 and 2008. Moreover, I deleted the variables which were derived from other variables, as their missing pattern would be exactly the same. I also deleted variables whose missingness is not meaningful (e.g. variable for additional notes, which is (a) optional and (b) text data).

```{r}
df16<-df.self %>%
  filter(year==2016) %>%
  select(-X,-case_id,-st,-dist,-pid3,-pid3_leaner,-turnout_selfreport,-state_po,
         -state_fips,-state_cen,-state_ic,-office,-Rpartyloss,-Rpartywins,-stable_resid_cons,
         -independent_strong,-independent_weak,-margin_003,-margin_002,-margin_001,
         -candidate,-party,-candidatevotes,-totalvotes,-notes,-share,-margin,-y_missing,
         -birthyr,-lostElection,-lossloss,-newsint_01,-newsint_012,-church1,-strong_partisan,-overallwin,-stable_resid,
         -overallwin,-year,-vep_highest_office,-version,-party7,-party_lean,-party3,-cd,-county_fips,-writein)

```

```{r}
#install.packages("mi")
#install.packages("betareg")
#install.packages("remotes")
library(dplyr)
library(mi)
library(betareg)
library(remotes)
remotes::install_github("cran/extracat")

colSums(is.na(df16)) %>%
  sort(decreasing= T) 

x<-missing_data.frame(df16)
image(x)

#x@patterns # commented out due to large data size.
#levels(x@patterns)
summary(x@patterns)

extracat::visna(df16, sort = "c")

```

As one can see immediately from the charts, the only variable with a substantial number of missing values is the *turnout_self01*, i.e. self-reported turnout, which is the dependent variable of interest. Out of the 5,783 observations, 916 are missing this value. The variable that has the next highest number of values is *pid7* i.e. political affiliation using 7 scales, with only 9 missing values. There are 4 additional variables with missing values (newint, residence, faminc) but they are in the single digits and given that these constitute a neglibile share of the sample, it would be easiest to drop these values from the analysis.

As for the missing turnout variable, given that this is the dependent variable of interest, I wouldn't feel comfortable imputing the value. As a result, I will drop the NAs by default in all illustrations unless otherwise indicated.

<!--chapter:end:04-missing.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Results

```{r}
self_turnout_state_rmNA<-df.self%>%
  filter(!is.na(turnout_self01))%>%
  group_by(state,year,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(turnout_self01==1)
```

```{r, echo=FALSE}
#data prep
table(df.self$turnout_self01)

breakdown<-df.self%>%
  group_by(state,year,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n))


breakdown.NA<-df.self%>%
  group_by(state,year,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(is.na(turnout_self01))

head(breakdown.NA)

breakdown.NA1<-df.self%>%
  group_by(state,year,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(turnout_self01==1)
```

```{r}
self_turnout_state08<-self_turnout_state_rmNA[self_turnout_state_rmNA$year==2008,]
self_turnout_state12<-self_turnout_state_rmNA[self_turnout_state_rmNA$year==2012,]
self_turnout_state16<-self_turnout_state_rmNA[self_turnout_state_rmNA$year==2016,]
```

## Are there state-level variations in over-reporting?
```{r}
#install.packages("choroplethrMaps")
#install.packages("choroplethr")
library(choroplethrMaps)
library(tidyverse)
library(choroplethr)

# data frame must contain "region" and "value" columns

df.map08 <- self_turnout_state08 %>% 
  as.data.frame() %>%
  transmute(region = tolower(`state`), value = freq)

df.map12 <- self_turnout_state12 %>% 
  as.data.frame() %>%
  transmute(region = tolower(`state`), value = freq)

df.map16 <- self_turnout_state16 %>% 
  as.data.frame() %>%
  transmute(region = tolower(`state`), value = freq)

state_choropleth(df.map08,
                 title = "2008",
                 legend = "Percent?")

state_choropleth(df.map12,
                 title = "2012",
                 legend = "Percent?")

state_choropleth(df.map16,
                 title = "2016",
                 legend = "Percent?")

```
For year 2016, it seems that over-reporting rates tended to be higher in coastal states, and relatively lower in the inner states. The pattern is not repeated in previous years, however, as the earlier maps indicate.

```{r}
# cleveland dot plot
theme_dotplot <- theme_bw(14) +
    theme(axis.text.y = element_text(size = rel(.75)),
        axis.ticks.y = element_blank(),
        axis.title.x = element_text(size = rel(.75)),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 0.5),
        panel.grid.minor.x = element_blank())

self_turnout_state_rmNA$year<-as.factor(self_turnout_state_rmNA$year)
ggplot(self_turnout_state_rmNA, aes(freq, fct_reorder2(state,year==2012, freq, .desc=F), 
                            color = year)) +
  geom_point() +
  ggtitle("% of sample claiming to have voted, by state") + ylab("") +
  theme_dotplot
```
The cleveland plot of overrporting rates allows us to see the range of over-reporting across states for each year, as well as to identify trends across years. While the share of the sample claiming to have voted does not form a tight band around the average at state-level, we can see that there is a pattern over time; over-reporting rates were much lower in 2008, whereas the blue dots lying to the right of green suggest that the values for 2016 in generally tended to be higher than in 2012.

```{r}
# share of NA's by state
ggplot(breakdown.NA, aes(freq, fct_reorder2(state,year==2012, freq, .desc=F), color = year)) +
  geom_point() +
  ggtitle("% of sample with missing values to Q about self-reported turnout", sub = "") + ylab("") +
  theme_dotplot
```
This plot shows that the share of respondents with missing values for the self-reported turntout (NA) shows a correlation with state. The green and blue dot roughly suggest that a state with a higher overreporting rate in 2012 also tended to have higher rate in 2016. Notice also the very low NA values for 2008. Many of the states also did not have a single NA, which is indicated by the missing red dot for at state level. This likely suggests that there was a change in the way the survey was conducted. It might not have been possible to proceed with the survey without answering the question in 2008, but that this practice changed in subsequent years. The map and the cleveland plots combined suggest that the state-level information aren't significant predictors of over-reporting. In any case, any discrepncies that are observed across states are likely to be attributable to administrative errors, rather than "cultural" variation in the likelihood that an individual would feel pressured to lie about not having voted. On that note, next we look at some of the demographic and political traits of individuals that may explain over-reporting.

## Demographic and political traits of respondents

```{r}
#install.packages("vcd")
library(vcd)
### change numbers to actual thins
# 1 democrat
# 2 rep 
# 3 ind
# 4 other
# 5 unsure

mosaic(turnout_self01~gender+educ,data=df.self,direction=c("v","v","h"),highlighting_fill = c("seashell","salmon1"))
```
Those with higher level of education are more likely than others to over-report turnout. While it's possible that this population also votes at a higher rate, the underlying data only look at those for whom there is no record of having voted, and the mosaic plot indicates that this pattern is found commonly in female as well as male respondents. We can also deduce that at each level of income, males over-report at slightly higher rate than do women, but that the difference across gender categories are smaller than the differences across educational levels.

overreporting by strong partisanship and party affiliation
```{r}
df3<-df.self[,c("strong_partisan","pid3","turnout_self01")]
df3<-df3[df3$pid3 %in% c(1,2),]

mosaic(turnout_self01~pid3+strong_partisan,data=df3,direction=c("v","v","h"),highlighting_fill = c("seashell","salmon1"))
```
Now we look at how overreporting rate differs by party affiliation, and  It shows that the overall pattern of over-reporting is similar for both female and male, but that on average, men are marginally more likely to overreport, regardless of their political affiliation. What is especially interesting is the fact that Independents are less likely than both Democrats and Republicans to overreport, and that overrepoting rate is especially low for those who responded that they were "Unsure" about their political affiliation. In other words, the stronger the political affiliation/identity, the more likely an individual is to say that they voted when in fact they did not. There may be a number of reasons for this behavior. Those who have strong interest in politics are likely to recognize the importance of voting, and conditional on not having voted, the guilt of not having done so and the embarrassment of others finding out would weigh in heavier on this population than others for whom it doesn't matter as much.

## Does the size of our social network predict over-reporting?

Similar to the demographic variables surveyed above, one would wonder, does having a large/strong social network influence how likely you are to over-report? While there are no perfect measures of social network, two variables included in the CCES stand out: (1) length of residency at current address and (2) frequency of church attendance. There are caveats that we need to take into consideration of course. If I moved to a new address, but that new location is wiithin my old neighborhood, having only lived there for a month doesn't mean I have weak ties to my neighbors.  Similarly, many people don't there are people who don't have any religious social network, but are nonetheless are very tightly embedded in their local social network. To deal with the second potential criticism, I limit the extrapolation I do on the church variable to "church-goers." In other words, whatever conclusion I draw shall be limited to those who would consider going to church at any point vs. those who are not Christians, or would never consider going to church.

```{r}
table(df.self$residence)

df.self$residence<-factor(df.self$residence,levels=c("Less than 1 month","1 to 6 months","7 to 11 months","1 to 2 years", "3 to 4 years","5 or more years"))

table(df.self$residence)
levels(df.self$residence)

residence<-df.self%>%
  group_by(residence,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(turnout_self01==1)

ggplot(residence, aes(freq, residence)) +
  geom_point() +
  ggtitle("% claiming to have voted (by length of residence)", sub = "NA is a separate category") +
  ylab("") +
  theme_dotplot


residence.NA<-df.self%>%
  group_by(residence,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(is.na(turnout_self01))

ggplot(residence.NA, aes(freq, reorder(residence, freq))) +
  geom_point() +
  ggtitle("% with missing values on self-reported turnout Q (by length of residence)", sub = "i.e. share of NAs") + ylab("") +
  theme_dotplot
```

```{r}
table(df.self$pew_churatd)

df.self$pew_churatd<-factor(df.self$pew_churatd,levels=c("More than once a week", "Once a week","Once or twice a month", "A few times a year","Seldom","Never", "Don't know"))

levels(df.self$pew_churatd)

church.NA<-df.self%>%
  group_by(pew_churatd,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(is.na(turnout_self01))

#ggplot(church.NA, aes(freq, reorder(pew_churatd, freq))) +
ggplot(church.NA, aes(freq, reorder(pew_churatd,desc(pew_churatd)))) +
  geom_point() +
  ggtitle("% with missing values on self-reported turnout Q (by church attendance)", sub = "i.e. share of NAs") + ylab("") +
  theme_dotplot

church.rmNA1<-df.self%>%
  filter(!is.na(turnout_self01)) %>%
  group_by(pew_churatd,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(turnout_self01==1)


ggplot(church.rmNA1, aes(freq, reorder(pew_churatd,desc(pew_churatd)))) +
  geom_point() +
  ggtitle("% over-reporting by church attendance", sub = "NAs not included") +
  ylab("") +
  theme_dotplot

```


```{r}
df_likert <- df.self %>% 
  dplyr::filter(pew_churatd!= 'NA'&pew_religimp != 'NA')

df_likert$pew_religimp <- factor(df_likert$pew_religimp, levels = c("Very important","Somewhat important", "Not too important","Not at all important"))

ggplot(df_likert) + 
 geom_bar(aes(x = reorder(pew_churatd,desc(pew_churatd)),fill = reorder(pew_religimp,desc(pew_religimp))), position = 'fill')+
  ylab('percentage')+
  xlab('church attendance frequency')+
  ggtitle('relationship b/w religiosity and church attendance')+
  coord_flip()+
  scale_fill_manual(values = c("seashell","lightsalmon1","salmon3","tomato4"))+
  labs(fill="religion's importance")
```

- EXPLAIN

* FYI: I wanted to have the "Very important" appear on the top of the legend, but I couldn't figure out a way to do it without changing the factor levels at the data frame level, which would also reverse the presentation of the chart, which is not what I wanted. So current listing is not ideal, but I kep it as is.


- d) by election cycle; general election including presidential vs. off yr etc. (election specific variables)
- e) turnout; margin of elections (This data will come from the MIT Elections Data) (if time allows)

<!--chapter:end:05-results.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Interactive component

text
new

<!--chapter:end:06-interactive.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Conclusion

hello, simple text

<!--chapter:end:07-conclusion.Rmd-->

