---
output:
  pdf_document: default
  html_document: default
---
# Results

```{r}
self_turnout_state_rmNA<-df.self%>%
  filter(!is.na(turnout_self01))%>%
  group_by(state,year,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(turnout_self01==1)
```

```{r, echo=FALSE}
#data prep
#table(df.self$turnout_self01)

breakdown<-df.self%>%
  group_by(state,year,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n))


breakdown.NA<-df.self%>%
  group_by(state,year,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(is.na(turnout_self01))

#head(breakdown.NA)

breakdown.NA1<-df.self%>%
  group_by(state,year,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(turnout_self01==1)
```

```{r}
self_turnout_state08<-self_turnout_state_rmNA[self_turnout_state_rmNA$year==2008,]
self_turnout_state12<-self_turnout_state_rmNA[self_turnout_state_rmNA$year==2012,]
self_turnout_state16<-self_turnout_state_rmNA[self_turnout_state_rmNA$year==2016,]
```

## Are there state-level variations in over-reporting?

Looking at state-level variations is interesting, first, because if there are any differences in administrative errors, administrative boundaries would be the logical place to look for it. After assuming away errors, however, it's also possible that certain cultural elements captured by states could influence whether overreporting is higher consistently in certain states over another. To see if there are interesting insight, I plot the overreporting rates by state. I visualize these on the map, as it'd be possible to detect if there are any geographic pattern to this phenomenon.

```{r}
#install.packages("choroplethrMaps")
#install.packages("choroplethr")
library(choroplethrMaps)
library(tidyverse)
library(choroplethr)

# data frame must contain "region" and "value" columns

df.map08 <- self_turnout_state08 %>% 
  as.data.frame() %>%
  transmute(region = tolower(`state`), value = freq*100)

df.map12 <- self_turnout_state12 %>% 
  as.data.frame() %>%
  transmute(region = tolower(`state`), value = freq*100)

df.map16 <- self_turnout_state16 %>% 
  as.data.frame() %>%
  transmute(region = tolower(`state`), value = freq*100)

state_choropleth(df.map08,
                 title = "2008",
                 legend = "%")

state_choropleth(df.map12,
                 title = "2012",
                 legend = "%")

state_choropleth(df.map16,
                 title = "2016",
                 legend = "%")

```

For year 2016, it seems that overreporting rates tended to be higher in coastal states, and relatively lower in the inner states. This would be an interesting pattern to illustrate, but unfortunately the same said pattern is not observed in previous years, as the maps from 2008 and 2012 indicate. Moreover, just based on the maps it's hard to tell if overreporting is consistent over years for a given state. To investigate this, we turn to cleveland plots.

** Ideally I would have adjusted the percentage scale to illustrate regular interval, but wasn't able to fix this.


```{r}
# cleveland dot plot
theme_dotplot <- theme_bw(14) +
    theme(axis.text.y = element_text(size = rel(.75)),
        axis.ticks.y = element_blank(),
        axis.title.x = element_text(size = rel(.75)),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 0.5),
        panel.grid.minor.x = element_blank())

self_turnout_state_rmNA$year<-as.factor(self_turnout_state_rmNA$year)
ggplot(self_turnout_state_rmNA, aes(freq, fct_reorder2(state,year==2012, freq, .desc=F), 
                            color = year)) +
  geom_point() +
  ggtitle("% of sample claiming to have voted, by state") + ylab("") +
  theme_dotplot + xlab ("percentage (in decimal)")
```

The cleveland plot of overrporting rates allows us to see the range of overreporting across all states for each election year, as well as to capture trends across years. While the share of the sample claiming to have voted does not form a tight band around the average at state-level, there does seem to be a temporal pattern; overreporting rates were much lower in 2008, whereas the blue dots lying to the right of green suggest that the values for 2016 in generally tended to be higher than in 2012. 

Next we look at the share of missing values to the self-reported turnout question. While this information does not directly speak to the phenomenon of overreporting per se, the fact that values are missing may be indicative of the unwillingness by respondents to provide an answer, a behavior that might be correlated also with the tendency to lie.

```{r}
# share of NA's by state
ggplot(breakdown.NA, aes(freq, fct_reorder2(state,year==2012, freq, .desc=F), color = year)) +
  geom_point() +
  ggtitle("% of sample with missing values to Q about self-reported turnout", sub = "") + ylab("") + xlab ("percentage (in decimal)")+
  theme_dotplot
```

This plot shows that the share of respondents with missing values for the self-reported turnout question (NA) are somewhat consistent over time within states. The green and blue dots roughly suggest that states with higher overreporting rates in 2012 also tended to have higher rates in 2016. Notice also the very low NA values for 2008. Many of the states in fact did not have a single NA, which is indicated by the missing red dot for many states. This likely suggests that there was a change in the way the survey was conducted. It might not have been possible to proceed with the survey without answering the question in 2008, and this practice may have changed in subsequent years. 

Together, the maps and the cleveland plots suggest that state-level information show some trends in administrative errors/overreporting but not a consistent enough pattern to support a statement one way or another. Based on the findings, it seems that any discrepancies that are observed across states are likely to be attributable to administrative errors, rather than state-level "cultural" variation in the likelihood that an individual would feel pressured to lie about not having voted. This would also explain why administrative errors/overreporting do not appear consistent within states over time--to the extent that state variations are explained more by admin errors, this would be highly contingent on the administrative capacity, which can vary depending on the pace of adopting new technology, and would be less constant over time than "cultural" characteristics. On that note, next we look at some of the demographic and political traits of individuals that may explain overreporting.

## Demographic and political traits of respondents

The dataset includes many demographic and political variables. Below, I sample just a few variables to visualize the relationship with overreporting.

```{r}
#install.packages("vcd")
library(vcd)

df.self1<-df.self %>%
  mutate(educ = 
           with(.,
                case_when(
                  educ ==1~"NoHS",
                  educ ==2~"HS",
                  educ ==3~"some_colg",
                  educ ==4~"2yr",
                  educ ==5~"4yr",
                  educ ==6~"postgrad")))

df.self1$educ<-factor(df.self1$educ,levels=c("NoHS", "HS","some_colg",
                                             "2yr","4yr","postgrad"))

df.self1$selfreported_turnout<-df.self1$turnout_self01
df.self1$selfreported_turnout<-ifelse(df.self1$selfreported_turnout==1,"Yes","No")

mosaic(selfreported_turnout~gender+educ,data=df.self1,direction=c("v","v","h"),highlighting_fill = c("seashell","salmon1"),main="overreporting by gender and education",labeling=labeling_border(
        rot_labels = c(0, 0, 60, 0), 
    #    just_labels=c("left","left","right","right"),
    #    tl_varnames = FALSE,
        gp_labels = gpar(fontsize = 9)))
```

The mosaic plot above shows that those with higher level of education are more likely than others to overreport turnout. (To the extent that the likelihood of administrative error in voting record should not differ by gender or voter's educational level, I assume that the difference is driven by individual propensity to lie, and not by admin error) While it's possible that the population with higher education also votes at a higher rate, recall that the underlying data is limited to those for whom there is no record of having voted, so we're looking at the tendency to lie, conditional on not having voted. An explanation for this pattern would be that individuals with higher level of education are (a) aware of the importance of voting as a civic duty, and (b) have peers who would expect them to vote. Hence, conditional on not having voted, the pressure to lie and say that they did may be higher. The mosaic plot indicates that this pattern is found in female as well as male respondents. We can also deduce that at each level of income, males over-report at slightly higher rates than do women, but that the difference across gender categories are smaller than the differences across educational levels.

** ideally the labels on the bottom should not encroach the actual mosaic plots.

```{r}
library(vcd)
df3<-df.self[,c("strong_partisan","pid3","turnout_self01","gender")]
df3<-df3[df3$pid3 %in% c(1,2,3,5),]
df3$party<-df3$pid3

df2<-df3 %>%
  mutate(party = 
           with(.,
                case_when(
                  pid3 ==1~"Dem",
                  pid3 ==2~"Rep",
                  pid3 ==3~"Ind",
                  pid3 ==5~"Unsure")))

df2$selfreported_turnout<-df2$turnout_self01
df2$selfreported_turnout<-ifelse(df2$selfreported_turnout==1,"Yes","No")
df2$strong_partisan<-ifelse(df2$strong_partisan==1,"Yes","No")

mosaic(selfreported_turnout~gender+party,data=df2,direction=c("v","v","h"),highlighting_fill = c("seashell","salmon1"), main="over-reporting by gender and party")
```

Now we look at how overreporting rate differs by gender and party affiliation.
What is noticeable is the fact that Independents are less likely than both Democrats and Republicans to overreport, and that overrepoting rate is even lower for those who responded that they were "Unsure" about their political affiliation. In other words, those with a political affiliation/identity are more likely to say that they voted when in fact they did not. Those who profess interest in politics and clearly identify with a party are likely to recognize the importance of voting, and conditional on not having voted, the guilt of not having done so and the embarrassment of others finding out would weigh in heavier on this population than others for whom it doesn't matter as much.

```{r}
df3<-df3[df3$pid3 %in% c(1,2),]
df3$party<-ifelse(df3$party==1,"Democrat","Republican")
df3$selfreported_turnout<-df3$turnout_self01
df3$selfreported_turnout<-ifelse(df3$selfreported_turnout==1,"Yes","No")
df3$strong_partisan<-ifelse(df3$strong_partisan==1,"Yes","No")

mosaic(selfreported_turnout~party+strong_partisan,data=df3,direction=c("v","v","h"),highlighting_fill = c("seashell","salmon1"), main="over-reporting by party and partisan strength")
```

This plot takes into account the strength of partisanship. It shows that while the overall pattern of overreporting is similar for Democrats and Republicans, Republicans are marginally more likely to overreport regardless of partisanship strength. As may be intuitive, those who claimed to be strong partisans, i.e. Strong Republicans or Strong Democrats, were also more likely to overreport. 

## Does the size of our social network predict over-reporting?

Similar to the demographic variables surveyed above, one would wonder, does having a large/strong social network influence how likely you are to overreport? When you have a lot of people who will count on you to vote, and you meet these people on frequent basis, and you value your relationship with them, are you more likely to feel pressured to lie about not having voted?

While there are no perfect measures of social network size in the CCES dataset, two variables can serve as proxies: (1) length of residency at current address and (2) frequency of church attendance. Those who have lived for a long time within a neighborhood would presumably have a larger local network size, i.e. peers who would matter in terms of putting pressure on them to vote. Similarly, those who are regular church attenders would presumably have more ties to other church members, holding friends and colleagues from other sources constant.

There are caveats that we need to take into consideration of course--these are not unequivocal measures of social network ties: If I moved to a new address, but that new location is wiithin my old neighborhood, having only lived there for a month doesn't mean I have weak ties to my neighbors.  Similarly, many people are not religious and therefore don't have any friends via religion, but are nonetheless are very tightly embedded in their local social network. To deal with the second potential criticism, I limit the scope of inference that can be made based on findings of the church variable. In other words, whatever conclusion I draw shall be limited to those who would ever consider going to church, and not be meaningful for understanding the pattern among non-Christians.

```{r}
library(ggplot2)
#table(df.self$residence)

df.self$residence<-factor(df.self$residence,levels=c("Less than 1 month","1 to 6 months","7 to 11 months","1 to 2 years", "3 to 4 years","5 or more years"))

residence<-df.self%>%
  filter(!is.na(residence))%>%
  group_by(residence,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(turnout_self01==1)

ggplot(residence, aes(freq, residence)) +
  geom_point() +
  ggtitle("% overreporting by length of residence") +
  ylab("") +xlab("%")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme_dotplot
```

The pattern in the plot is quite clear. The longer one has been at the current address, the more likely they were to overreport. This is a striking result, because it means that the effect of lying offsets the possible effects of administrative errors. As explained earlier, errors are most likely to arise for people whose records have changed recently. This would mean that those who are in the "Less than 1 month" or "7 to 11 months" categories should show higher rates of overreporting, but the plot shows in fact that the opposite is true, supporting the hypothesis that strong local ties can increase the likelihood of lying out of social desirability bias.

```{r}
#table(df.self$pew_churatd)

df.self$pew_churatd<-factor(df.self$pew_churatd,levels=c("More than once a week", "Once a week","Once or twice a month", "A few times a year","Seldom","Never", "Don't know"))

#levels(df.self$pew_churatd)

# church.NA<-df.self%>%
#   group_by(pew_churatd,turnout_self01) %>%
#   summarise(n=n())%>%
#   mutate(freq=n/sum(n)) %>%
#   filter(is.na(turnout_self01))
# 
# ggplot(church.NA, aes(freq, reorder(pew_churatd,desc(pew_churatd)))) +
#   geom_point() +
#   ggtitle("% with missing values on self-reported turnout Q (by church attendance)", sub = "i.e. share of NAs") + ylab("") +
#   theme_dotplot

church.rmNA1<-df.self%>%
  filter(!is.na(pew_churatd))%>%
  filter(!is.na(turnout_self01)) %>%
  group_by(pew_churatd,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(turnout_self01==1)

ggplot(church.rmNA1, aes(freq, reorder(pew_churatd,desc(pew_churatd)))) +
  geom_point() +
  ggtitle("% over-reporting by church attendance") +
  ylab("") +xlab("%")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme_dotplot
```

A similar pattern is observed for church attendance. Among those who went to church regularly (more than once a month), overreporting rates are higher. You may be wondering about whether church attendance is really a good proxy. Wouldn't it be possible that those who are more religious also feel more guilty if they didn't manage to vote? And as a result, if asked, wouldn't this pressure make it more likely that they lie? In other words, the source of pressure is not social network size, but religiosity? That would be a valid criticism. I cannot alleviate all doubts, but there are two things to keep in mind in response to that question. First, a level of religiosity strong enough to create guilt would have also motivated individuals to actually go out and vote. We do not know if voter turnout varies by religiosity, and that's not something I can address with the data at hand. What I *can* show, though is how religiosity and church attendance are related. The stacked bar chart below illustrate the point.

```{r}
# df_likert <- df.self %>%
#   filter(pew_churatd!= 'NA' & pew_religimp != 'NA')
df_likert <- df.self %>%
  filter(!is.na(pew_churatd) & !is.na(pew_religimp))

df_likert$pew_religimp <- factor(df_likert$pew_religimp, levels = c("Very important","Somewhat important", "Not too important","Not at all important"))

ggplot(df_likert) + 
 geom_bar(aes(x = reorder(pew_churatd,desc(pew_churatd)),fill = reorder(pew_religimp,desc(pew_religimp))), position = 'fill')+
  ylab('percent share')+
  xlab('church attendance frequency')+
  ggtitle('relationship b/w religiosity and church attendance')+
  coord_flip()+
  scale_fill_manual(values = c("seashell","lightsalmon1","salmon3","tomato4"))+
  labs(fill="religion's importance")
```

As expected, religiosity and church attendance are strongly correlated. Note, however, even for those who "Seldom" go to church, over 50 % of them say that religion is either "Very" or "Somewhat" important. To obtain a definitve answer to whether church attendance has a differential impact on overreporting *independent* of religiosity would therefore require statistical analyses that would be a natural next step for this project.

** FYI: I wanted to have the "Very important" appear first in the legend, but I couldn't figure out a way to do it without changing the factor levels at the data frame level, which would also reverse the presentation of the chart, which is not what I wanted. So current listing is not ideal, but I kept it as is.

```{r,echo=FALSE}
df.gender<-df.self%>%
  group_by(gender,year,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(turnout_self01==1)

df.race<-df.self%>%
  group_by(race,year,turnout_self01) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  filter(turnout_self01==1)
  
```

