[["index.html", "Self-reported voter turnout Chapter 1 Introduction", " Self-reported voter turnout Jiyeon Chang 2020-12-19 Chapter 1 Introduction Average turnout rates in US elections hover around 50-60%, but in surveys, the figure is often closer to 70-80%. Why is that? It’s possible that this gap is attributable to nonresponse bias, but survey researchers report that social desirability bias also plays a role. Social desirability bias is the tendency among survey respondents to choose an answer that they believe to be good, rather than one that reflects their true beliefs or behaviors. In the case of turnout, this means that there are people who lie about whether they voted, i.e. they say they voted even though in reality they did not. This is a tricky issue for survey researchers, as it introduces bias in the data, leading to overreporting. In this project, I look at what are some of the variables that may explain the phenomenon of overreporting. The main dataset I use to explore this question is the Cooperative Congressional Election Study (https://cces.gov.harvard.edu/). The survey asks respondents whether they voted in the recent election. For each respondent, there is a variable indicating whether there exists a record of that person of having voted. The strategy here is to look at respondents for whom there is no record of voting, i.e. whom we can assume to not have voted. Once we have this subsample, we can compare the group of people that claims to have voted (record does not match what they say) vs. those who say that they did not vote (their statement matches the missing record). One word of caution: For those who claim to have voted, there are two possible explanations for why there might not be a corresponding voting record for them. First, it may have been an administrative error–a common reason for this is that the person moved recently, and that their voting record is not up-to-date. In this case, the missing record is not an indication that they are lying. Another possibility, however, is that the respondent is be lying. There is no definitive way to distinguish which of the missing records are due to error vs. lying. However, if we can assume that the rate of administrative error should be random across different subsamples of interests, then it is possible to examine which groups are more likely to over-report turnout. To this end, I present the findings based on the following three questions: Does overreporting vary by state, and if so, is there a useful pattern there? How does overreporting vary by key demographic and political traits? Does having strong/large social networks increase the probability that an individual will overreport, conditional on not having voted? "],["data-sources.html", "Chapter 2 Data sources", " Chapter 2 Data sources The main dataset I use is the Cooperative Congressional Election Study (https://cces.gov.harvard.edu/). CCES is a 50,000+ person national stratified sample survey administered by YouGov on a biannual basis. For the purpose of this project, I look at the 3 most recent presidential elections for which data is available–that is, 2008, 2012 and 2016. The data from 2016, for instance, includes 64,600 observations and 563 variables. However, I look a subset of this sample, namely those for whom there is no voting record. For the 3 years, this reduces the sample to a total of 42,305 observations. A wide range of variables are included in the dataset, including the main demographic variables (age, gender, state/county of residence, race, education, income, etc.) and an extensive list of questions about political affiliation and attitudes, support for specific candidates and opinions about proposed policies, such as climate change, and gun control. As briefly mentioned above, one of the challenges of using this data is that, for those who claim to have voted (but for whom there is no matching voting record), we won’t ever know whether these people are lying or if there’s been an administrative error. Literature pertaining to the field identifies a number of factors that may predict a higher probability of admin error, e.g. recent change in residence. There are many variables for which the rate of administrate error should not differ across the response categories, e.g. gender, or religion. This is more a problem with the conceptualization of the question rather than a problem with the quality of data, however, and I will address the limitations in the conclusions one can draw from the visualization that I will produce. "],["data-transformation.html", "Chapter 3 Data transformation", " Chapter 3 Data transformation The data has been prepared by merging data from 3 election years, and also merging the CCES data with electoral outcome dataset to create variables accounting for state level turnout and margin of victory. It takes a while to load and clean the data, so data prepartion was done in R in the begining and this project loads the clean dataset saved locally. The main steps I undertook in data cleaning involved the following: Rename variables from each year to prepare for the merger. A similar set of questions were asked each year but they didn’t necessarily have the same variable names. Recode variables with several levels into binary variables where it is helpful: This is done for variables such as church01, which is 1 if the person goes to church at least several times a year and 0 otherwise. (The original variable had more detailed frequencies.) Similar steps were taken to indicate stable_residency01, which is 1 if the person has been residing at current address for &gt;1 year. The electoral outcome at state-year level from the MIT Elections data (https://electionlab.mit.edu/data) was merged with the respondent’s state-year to create variables indicating how big the margin was in the race they voted, and which party won. These variables make it possible to capture the “political reality” of the electoral unit (state) in which each respondent is voting. (In the end this portion of the data was not used for the final results, but can be used for additional analyses in the future) "],["missing-values.html", "Chapter 4 Missing values", " Chapter 4 Missing values My dataset is quite large, so for the purpose of visualization, I restrict the analysis to 2016, and assume that the pattern will be similar for years 2012 and 2008. Moreover, I deleted the variables which were derived from other variables, as their missing pattern would be exactly the same. I also deleted variables whose missingness is not meaningful (e.g. variable for additional notes, which is (a) optional and (b) text data). As one can see immediately from the charts, the only variable with a substantial number of missing values is the turnout_self01, i.e. self-reported turnout, which is the dependent variable of interest. The other variables with some missing data constitute such a small share of the sample that their aggregate measure barely appear on the plot. Given their negligible share, it would be easiest to drop these values from the analysis. As for the missing turnout variable, given that this is the dependent variable of interest, I wouldn’t feel comfortable imputing the value. As a result, I will drop the NAs by default in all illustrations unless otherwise indicated. "],["results.html", "Chapter 5 Results 5.1 Are there state-level variations in over-reporting? 5.2 Demographic and political traits of respondents 5.3 Does the size of our social network predict over-reporting?", " Chapter 5 Results Note on interpreting what overreporting % means: overreporting rate of 60% means that of the people for whom there is no record of having voted, 60% of them say that they voted. 5.1 Are there state-level variations in over-reporting? Looking at state-level variations is interesting, first, because if there are any differences in administrative errors, administrative boundaries would be the logical place to look for it. After assuming away errors, however, it’s also possible that certain cultural elements captured by states could influence whether overreporting is higher consistently in certain states over another. To see if there are interesting insight, I plot the overreporting rates by state. I visualize these on the map, as it’d be possible to detect if there are any geographic pattern to this phenomenon. For year 2016, it seems that overreporting rates tended to be higher in coastal states, and relatively lower in the inner states. This would be an interesting pattern to illustrate, but unfortunately the same said pattern is not observed in previous years, as the maps from 2008 and 2012 indicate. Moreover, just based on the maps it’s hard to tell if overreporting is consistent over years for a given state. To investigate this, we turn to cleveland plots. ** Ideally I would have adjusted the percentage scale to illustrate regular interval, but wasn’t able to fix this. The cleveland plot of overrporting rates allows us to see the range of overreporting across all states for each election year, as well as to capture trends across years. While the share of the sample claiming to have voted does not form a tight band around the average at state-level, there does seem to be a temporal pattern; overreporting rates were much lower in 2008, whereas the blue dots lying to the right of green suggest that the values for 2016 in generally tended to be higher than in 2012. Next we look at the share of missing values to the self-reported turnout question. While this information does not directly speak to the phenomenon of overreporting per se, the fact that values are missing may be indicative of the unwillingness by respondents to provide an answer, a behavior that might be correlated also with the tendency to lie. This plot shows that the share of respondents with missing values for the self-reported turnout question (NA) are somewhat consistent over time within states. The green and blue dots roughly suggest that states with higher overreporting rates in 2012 also tended to have higher rates in 2016. Notice also the very low NA values for 2008. Many of the states in fact did not have a single NA, which is indicated by the missing red dot for many states. This likely suggests that there was a change in the way the survey was conducted. It might not have been possible to proceed with the survey without answering the question in 2008, and this practice may have changed in subsequent years. Together, the maps and the cleveland plots suggest that state-level information show some trends in administrative errors/overreporting but not a consistent enough pattern to support a statement one way or another. Based on the findings, it seems that any discrepancies that are observed across states are likely to be attributable to administrative errors, rather than state-level “cultural” variation in the likelihood that an individual would feel pressured to lie about not having voted. This would also explain why administrative errors/overreporting do not appear consistent within states over time–to the extent that state variations are explained more by admin errors, this would be highly contingent on the administrative capacity, which can vary depending on the pace of adopting new technology, and would be less constant over time than “cultural” characteristics. On that note, next we look at some of the demographic and political traits of individuals that may explain overreporting. 5.2 Demographic and political traits of respondents The dataset includes many demographic and political variables. Below, I sample just a few variables to visualize the relationship with overreporting. The mosaic plot above shows that those with higher level of education are more likely than others to overreport turnout. (To the extent that the likelihood of administrative error in voting record should not differ by gender or voter’s educational level, I assume that the difference is driven by individual propensity to lie, and not by admin error) While it’s possible that the population with higher education also votes at a higher rate, recall that the underlying data is limited to those for whom there is no record of having voted, so we’re looking at the tendency to lie, conditional on not having voted. An explanation for this pattern would be that individuals with higher level of education are (a) aware of the importance of voting as a civic duty, and (b) have peers who would expect them to vote. Hence, conditional on not having voted, the pressure to lie and say that they did may be higher. The mosaic plot indicates that this pattern is found in female as well as male respondents. We can also deduce that at each level of income, males over-report at slightly higher rates than do women, but that the difference across gender categories are smaller than the differences across educational levels. ** ideally the labels on the bottom should not encroach the actual mosaic plots. Now we look at how overreporting rate differs by gender and party affiliation. What is noticeable is the fact that Independents are less likely than both Democrats and Republicans to overreport, and that overrepoting rate is even lower for those who responded that they were “Unsure” about their political affiliation. In other words, those with a political affiliation/identity are more likely to say that they voted when in fact they did not. Those who profess interest in politics and clearly identify with a party are likely to recognize the importance of voting, and conditional on not having voted, the guilt of not having done so and the embarrassment of others finding out would weigh in heavier on this population than others for whom it doesn’t matter as much. This plot takes into account the strength of partisanship. It shows that while the overall pattern of overreporting is similar for Democrats and Republicans, Republicans are marginally more likely to overreport regardless of partisanship strength. As may be intuitive, those who claimed to be strong partisans, i.e. Strong Republicans or Strong Democrats, were also more likely to overreport. 5.3 Does the size of our social network predict over-reporting? Similar to the demographic variables surveyed above, one would wonder, does having a large/strong social network influence how likely you are to overreport? When you have a lot of people who will count on you to vote, and you meet these people on frequent basis, and you value your relationship with them, are you more likely to feel pressured to lie about not having voted? While there are no perfect measures of social network size in the CCES dataset, two variables can serve as proxies: (1) length of residency at current address and (2) frequency of church attendance. Those who have lived for a long time within a neighborhood would presumably have a larger local network size, i.e. peers who would matter in terms of putting pressure on them to vote. Similarly, those who are regular church attenders would presumably have more ties to other church members, holding friends and colleagues from other sources constant. There are caveats that we need to take into consideration of course–these are not unequivocal measures of social network ties: If I moved to a new address, but that new location is wiithin my old neighborhood, having only lived there for a month doesn’t mean I have weak ties to my neighbors. Similarly, many people are not religious and therefore don’t have any friends via religion, but are nonetheless are very tightly embedded in their local social network. To deal with the second potential criticism, I limit the scope of inference that can be made based on findings of the church variable. In other words, whatever conclusion I draw shall be limited to those who would ever consider going to church, and not be meaningful for understanding the pattern among non-Christians. The pattern in the plot is quite clear. The longer one has been at the current address, the more likely they were to overreport. This is a striking result, because it means that the effect of lying offsets the possible effects of administrative errors. As explained earlier, errors are most likely to arise for people whose records have changed recently. This would mean that those who are in the “Less than 1 month” or “7 to 11 months” categories should show higher rates of overreporting, but the plot shows in fact that the opposite is true, supporting the hypothesis that strong local ties can increase the likelihood of lying out of social desirability bias. A similar pattern is observed for church attendance. Among those who went to church regularly (more than once a month), overreporting rates are higher. You may be wondering about whether church attendance is really a good proxy. Wouldn’t it be possible that those who are more religious also feel more guilty if they didn’t manage to vote? And as a result, if asked, wouldn’t this pressure make it more likely that they lie? In other words, the source of pressure is not social network size, but religiosity? That would be a valid criticism. I cannot alleviate all doubts, but there are two things to keep in mind in response to that question. First, a level of religiosity strong enough to create guilt would have also motivated individuals to actually go out and vote. We do not know if voter turnout varies by religiosity, and that’s not something I can address with the data at hand. What I can show, though is how religiosity and church attendance are related. The stacked bar chart below illustrate the point. As expected, religiosity and church attendance are strongly correlated. Note, however, even for those who “Seldom” go to church, over 50 % of them say that religion is either “Very” or “Somewhat” important. To obtain a definitve answer to whether church attendance has a differential impact on overreporting independent of religiosity would therefore require statistical analyses that would be a natural next step for this project. ** FYI: I wanted to have the “Very important” appear first in the legend, but I couldn’t figure out a way to do it without changing the factor levels at the data frame level, which would also reverse the presentation of the chart, which is not what I wanted. So current listing is not ideal, but I kept it as is. "],["interactive-component.html", "Chapter 6 Interactive component", " Chapter 6 Interactive component For the interactive component, I build a d3 object where the over-reporting rate can be compared across years for select variables: gender and race. In the main results sections, I avoided presenting multiple identical plots that varied only in time, except for the map plots. Instead, I aggregated all data points across years. The assumption there was that the pattern in each year would be broadly consistent with the aggregate pattern. It is possible, however, that this fails to capture interesting insight that we can unearth only by looking at changes over time. To illustrate the point, I created bar charts that change when the user chooses a different year to view. The default visualization uses data from 2008. See for visualization of gender differences: https://vizhub.com/mandu427/6cc35a21157e482b8f354a65ca5b6c2d?edit=files&amp;file=index.html&amp;line&amp;mode=full Note that I had two ways of visualizing this: (1) holding the y scale constant and allowing the height of bars to vary and, (2) varying the y scale depending on the maximum values of the data. My visualization does (2). The advantage of setting the maximum value of the y scale as the maximum y value in the data for each year, is that it makes it easier to focus on the difference between Female and Male. Of course, an attentive audience can also take note of the changing overall levels of over-reporting, by looking at the numbers on the scale. On the other hand, had I done (1) and fixed the y scale to a set number, it would have been easier to spot the changes in overall levels (for both Female and Male), but it may be harder to see the changes in the difference between the two categories, especially if the difference is marginal. In this case, the gender plot shows that overall levels of overreporting are higher in years 2012 and 2016 compared to 2008. The gap between Female and Male increased in 2012, but returns to similar levels in 2016. In this case there isn’t a clear pattern over time, and it’s possible that the difference is statistically not significant. This could be an area of further analysis. https://vizhub.com/mandu427/268ba661bb3d4f0ba82188fa0a60d689?edit=files&amp;file=index.html&amp;line&amp;mode=full The second link produces a similar plot, but for race. Again, I use the maximum y value in each year as the maximum value of the y scale, which makes it easier how the relative differences across racial groups change over time. From these visualizations it’s quite easy to see that the over-reporting rate among Asians has increased relative to the share of Whites, from 2008 to 2012, and again in 2016. I don’t have an a priori theory about this change, and it’s important to note that it’s also possible that some of the change is due to sampling, since the number of observations for Asians (e.g. 612 in year 2016) is much smaller than for Whites (9,710 in 2016). However, the change seems to be substantial, and it would be interesting to investigate the reasons behind this. The code for the interactive visualizations were based on modifications of this code: https://www.d3-graph-gallery.com/graph/barplot_button_data_hard.html "],["conclusion.html", "Chapter 7 Conclusion", " Chapter 7 Conclusion The main limitations of this project–related to the fact that we cannot distinguish between administrative errors and lies–were discussed at the outset. Despite this challenge, it is possible, at least for variables that I examined, to identify a few patterns that predict overreporting. It would be interesting, time permitting, to look at additional political variables, such as margins of the elections, which party won, etc. to see if these electoral outcomes also have an effect on overreporting. The challenge here though would be that electoral outcome would be identical for all respondents within electoral administrative units (i.e. state, congressional district (CD)), and given that the same party consistently wins many states, there may not be sufficient variation in electoral outcomes at state level (although more variation would be seen at CD level) with data from only 3 years. The main challenge I encountered writing up the results was figuring out how to communicate the “findings.” In statistics, we often speak in terms of p-values and significance levels, so I found myself keep asking whether the pattern I was seeing was “clear enough” or if the differences in groups were really “noticeable.” As we learned in the course, scales can really make a difference, and while it’s tempting to opt for “findings,” it’s important to avoid presenting patterns where there is none. All of this is just to say that it’s been great learning complementarity of dataviz and statistics! "]]
