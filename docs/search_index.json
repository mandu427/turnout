[["index.html", "Self-reported voter turnout Chapter 1 Introduction", " Self-reported voter turnout Jiyeon Chang 2020-12-19 Chapter 1 Introduction Average turnout in US elections hover around 50-60%, but in surveys, the figure is often closer to 70-80%. Why is that? It’s possible that this gap is attributable to nonresponse bias, but survey researchers report that partly responsible is also an element of social desirability bias, which may influence some people to respone to surveys in ways that do not reflect the truth. In the case of turnout, this means that there are people who lie about whether they voted, i.e. they say they voted even though in reality they did not. This is a tricky issue for survey researchers, as it introduces bias in the data. In this project, I look at what are some of the variables that may explain over-reporting. The main dataset I use to explore this question is the Cooperative Congressional Election Study (https://cces.gov.harvard.edu/). The survey asks respondents whether they voted in the recent election. For each respondent, there is a variable indicating whether there exists a record of that person of having voted. So the strategy here is to look at respondents for whom there is no record of voting, whom we can assume to not have voted. Once we have this subsample, we can compare the group of people that claims to have voted (record does not match what they say) vs. those who say that they did not vote (their statement matches the missing record). One word of caution: For those who claim to have voted, there are two possible explanations for why there might not be a corresponding voting record for them. First, it may have been an administrative error–a common reason for this is that the person moved recently, and that their voting record is not up-to-date. In this case, the missing record is not an indication that they are lying. Alternatively, the respondent may be lying. There is no definitive way to distinguish which of the missing records are due to error vs. lying. However, if we can assume that the rate of administrative error should be random across different subsamples of interests, then it is possible to examine based on existing data, what groups are more likely to over-report turnout. To this end, I present the findings based on the following three questions: Does the over-reporting pattern vary by state, and if so, is there a useful pattern there? How does over-reporting vary by key demographic and political traits? Does having strong/large social networks increase the probability that an individual will over-report, conditional on not having voted? "],["data-sources.html", "Chapter 2 Data sources", " Chapter 2 Data sources The main dataset I use is the Cooperative Congressional Election Study (https://cces.gov.harvard.edu/). CCES is a 50,000+ person national stratified sample survey administered by YouGov on a biannual basis. For the purpose of this project, I look at the 3 most recent presidential elections for which data is available–that is, 2008, 2012 and 2016. The data from 2016, for instance, includes 64,600 observations and 563 variables. However, I look a subset of this sample, namely those for whom there is no voting record. For the 3 years, this reduces the sample to a total of 42,305 observations. A wide range of variables are included in the dataset, including the main demographic variables (age, gender, state/county of residence, race, education, income, etc.) and an extensive list of questions about political affiliation and attitudes, support for specific candidates and opinions about proposed policies, such as climate change, and gun control. As briefly mentioned above, one of the challenges of using this data is that, for those who claim to have voted (but for whom there is no matching voting record), we won’t ever know whether these people are lying or if there’s been an administrative error. Literature pertaining to the field identifies a number of factors that may predict a higher probability of admin error, e.g. recent change in residence. There are many variables for which the rate of administrate error should not differ across the response categories, e.g. gender, or religion. This is more a problem with the conceptualization of the question rather than a problem with the quality of data, however, and I will address the limitations in the conclusions one can draw from the visualization that I will produce. "],["data-transformation.html", "Chapter 3 Data transformation", " Chapter 3 Data transformation The data has been prepared by merging data from 3 election years, and also merging the CCES data with electoral outcome dataset to create variables accounting for state level turnout and margin of victory. It takes a while to load and clean the data, so data prepartion was done in R in the begining and this project loads the clean dataset saved locally. The main steps I undertook in data cleaning involved the following: Rename variables from each year to prepare for the merger. A similar set of questions were asked each year but they didn’t necessarily have the same variable names. Recode variables with several levels into binary variables where it is helpful: This is done for variables such as church01, which is 1 if the person goes to church at least several times a year and 0 otherwise. (The original variable had more detailed frequencies.) Similar steps were taken to indicate stable_residency01, which is 1 if the person has been residing at current address for &gt;1 year. The electoral outcome at state-year level from the MIT Elections data (https://electionlab.mit.edu/data) was merged with the respondent’s state-year to create variables indicating how big the margin was in the race they voted, and which party won. These variables make it possible to capture the “political reality” of the electoral unit (state) in which each respondent is voting. (In the end this portion of the data was not used for the final results, but can be used for additional analyses in the future) "],["missing-values.html", "Chapter 4 Missing values", " Chapter 4 Missing values My dataset is quite large, so for the purpose of visualization, I restrict the analysis to 2016, and assume that the pattern will be similar for years 2012 and 2008. Moreover, I deleted the variables which were derived from other variables, as their missing pattern would be exactly the same. I also deleted variables whose missingness is not meaningful (e.g. variable for additional notes, which is (a) optional and (b) text data). As one can see immediately from the charts, the only variable with a substantial number of missing values is the turnout_self01, i.e. self-reported turnout, which is the dependent variable of interest. The other variables with some missing data constitute such a small share of the sample that their aggregate measure barely appear on the plot. Given their negligible share, it would be easiest to drop these values from the analysis. As for the missing turnout variable, given that this is the dependent variable of interest, I wouldn’t feel comfortable imputing the value. As a result, I will drop the NAs by default in all illustrations unless otherwise indicated. "],["results.html", "Chapter 5 Results 5.1 Are there state-level variations in over-reporting? 5.2 Demographic and political traits of respondents 5.3 Does the size of our social network predict over-reporting?", " Chapter 5 Results 5.1 Are there state-level variations in over-reporting? Looking at state-level variations is interesting, first, because if there are any differences in administrative errors, administrative boundaries would be the logical place to look for it. After assuming away errors, however, it’s also possible that certain cultural elements captured by states could influence whether overreporting is higher consistently in certain states over another. To see if there are interesting insight, I plot the overreporting rate by state. I visualize these on the map, as it’d be possible to detect if there are any geographic pattern to this phenomenon. For year 2016, it seems that overreporting rates tended to be higher in coastal states, and relatively lower in the inner states. This would be an interesting pattern to illustrate, but unfortunately the same said pattern is not observed in previous years, as the maps from 2008 and 2012 indicate. Moreover, just based on the maps it’s hard to tell if overreporting is consistent over years for a given state. To investigate this, we turn to cleveland plots. ** Ideally I would have adjusted the percentage scale to illustrate regular interval, but wasn’t able to fix this. The cleveland plot of overrporting rates allows us to see the range of overreporting across all states for each election year, as well as to capture trends across years. While the share of the sample claiming to have voted does not form a tight band around the average at state-level, there does seem to be a temporal pattern; overreporting rates were much lower in 2008, whereas the blue dots lying to the right of green suggest that the values for 2016 in generally tended to be higher than in 2012. Next we look at the share of missing values to the self-reported turnout question. While this information does not directly speak to the phenomenon of overreporting per se, the fact that values are missing may be indicative of the unwillingness by respondents to provide an answer, a behavior that might be correlated also with the tendency to lie. This plot shows that the share of respondents with missing values for the self-reported turnout question (NA) are somewhat consistent over time within states. The green and blue dots roughly suggest that states with higher overreporting rates in 2012 also tended to have higher rates in 2016. Notice also the very low NA values for 2008. Many of the states in fact did not have a single NA, which is indicated by the missing red dot for many states. This likely suggests that there was a change in the way the survey was conducted. It might not have been possible to proceed with the survey without answering the question in 2008, and this practice may have changed in subsequent years. Together, the maps and the cleveland plots suggest that state-level information show some trends in administrative errors/overreporting but not a consistent enough pattern to support a statement one way or another. Based on the findings, it seems that any discrepncies that are observed across states are likely to be attributable to administrative errors, rather than “cultural” variation in the likelihood that an individual would feel pressured to lie about not having voted. This would also explain why administrative errors/overreporting do not appear consistent within states over time–to the extent that state variations are explained more by admin errors, this would be highly contingent on the administrative capacity, which can vary depending on the pace of new technology, and would be less sticky than say, “cultural” characteristics. On that note, next we look at some of the demographic and political traits of individuals that may explain overreporting. 5.2 Demographic and political traits of respondents The dataset includes many demographic and political variables. Below, I sample just a few examples to illustrate interesting points. The mosaic plot above shows that those with higher level of education are more likely than others to overreport turnout. (To the extent that the likelihood of administrative error in voting record should not differ by gender or voter’s educational level, I assume that the difference is driven by individual propensity to lie, and not by admin error) While it’s possible that the population with higher education also votes at a higher rate, recall that the underlying data is limited to those for whom there is no record of having voted, so we’re looking at the tendency to lie, conditional on not having voted. An explanation for this pattern would be that individuals with higher level of education are (a) aware of the importance of voting as a civic duty, and (b) have peers who would expect them to vote. Hence, conditional on not having voted, the pressure to lie and say that they did would be higher. The mosaic plot indicates that this pattern is found in female as well as male respondents. We can also deduce that at each level of income, males over-report at slightly higher rates than do women, but that the difference across gender categories are smaller than the differences across educational levels. ** ideally the labels on the bottom should not encroach the actual mosaic plots. Now we look at how overreporting rate differs by gender and party affiliation. What is noticeable is the fact that Independents are less likely than both Democrats and Republicans to overreport, and that overrepoting rate is even lower for those who responded that they were “Unsure” about their political affiliation. In other words, the stronger the political affiliation/identity, the more likely an individual is to say that they voted when in fact they did not. There may be a number of reasons for this behavior. Those who have strong interest in politics are likely to recognize the importance of voting, and conditional on not having voted, the guilt of not having done so and the embarrassment of others finding out would weigh in heavier on this population than others for whom it doesn’t matter as much. This plot takes into account the strength of partisanship. It shows that while the overall pattern of overreporting is similar for Democrats and Republicans, Republicans are marginally more likely to overreport regardless of partisanship strength. As may be intuitive, those who claimed to be strong partisans, i.e. Strong Republicans or Strong Democrats, were also more likely to overreport. 5.3 Does the size of our social network predict over-reporting? Similar to the demographic variables surveyed above, one would wonder, does having a large/strong social network influence how likely you are to over-report? While there are no perfect measures of social network, two variables included in the CCES stand out: (1) length of residency at current address and (2) frequency of church attendance. There are caveats that we need to take into consideration of course. If I moved to a new address, but that new location is wiithin my old neighborhood, having only lived there for a month doesn’t mean I have weak ties to my neighbors. Similarly, many people don’t there are people who don’t have any religious social network, but are nonetheless are very tightly embedded in their local social network. To deal with the second potential criticism, I limit the extrapolation I do on the church variable to “church-goers.” In other words, whatever conclusion I draw shall be limited to those who would consider going to church at any point vs. those who are not Christians, or would never consider going to church. The above plot shows that indeed those who are frequent church goes are also more religious. However, even for those who go to church only seldom, more than half of the respondents from this group say that religion is very or somewhat important. ** FYI: I wanted to have the “Very important” appear on the top of the legend, but I couldn’t figure out a way to do it without changing the factor levels at the data frame level, which would also reverse the presentation of the chart, which is not what I wanted. So current listing is not ideal, but I kept it as is. "],["interactive-component.html", "Chapter 6 Interactive component", " Chapter 6 Interactive component For the interactive component, I build a d3 object where the over-reporting rate can be compared across years for select variables: gender and race. In the main results sections, I avoided presenting multiple identical plots that varied only in time, except for the map plots. Instead, I aggregated all data points across years. The assumption there was that the pattern in each year would be broadly consistent with the aggregate pattern. It is possible, however, that this fails to capture interesting insight that we can unearth only by looking at changes over time. To illustrate the point, I created bar charts that change when the user chooses a different year to view. The default visualization uses data from 2008. See for visualization of gender differences: https://vizhub.com/mandu427/6cc35a21157e482b8f354a65ca5b6c2d?edit=files&amp;file=index.html&amp;line&amp;mode=full Note that I had two ways of visualizing this: (1) holding the y scale constant and allowing the height of bars to vary and, (2) varying the y scale depending on the maximum values of the data. My visualization does (2). The advantage of setting the maximum value of the y scale as the maximum y value in the data for each year, is that it makes it easier to focus on the difference between Female and Male. Of course, an attentive audience can also take note of the changing overall levels of over-reporting, by looking at the numbers on the scale. On the other hand, had I done (1) and fixed the y scale to a set number, it would have been easier to spot the changes in overall levels (for both Female and Male), but it may be harder to see the changes in the difference between the two categories, especially if the difference is marginal. In this case, the gender plot shows that overall levels of overreporting are higher in years 2012 and 2016 compared to 2008. The gap between Female and Male increased in 2012, but returns to similar levels in 2016. In this case there isn’t a clear pattern over time, and it’s possible that the difference is statistically not significant. This could be an area of further analysis. https://vizhub.com/mandu427/268ba661bb3d4f0ba82188fa0a60d689?edit=files&amp;file=index.html&amp;line&amp;mode=full The second link produces a similar plot, but for race. Again, I use the maximum y value in each year as the maximum value of the y scale, which makes it easier how the relative differences across racial groups change over time. From these visualizations it’s quite easy to see that the over-reporting rate among Asians has increased relative to the share of Whites, from 2008 to 2012, and again in 2016. I don’t have an a priori theory about this change, and it’s important to note that it’s also possible that some of the change is due to sampling, since the number of observations for Asians (e.g. 612 in year 2016) is much smaller than for Whites (9,710 in 2016). However, the change seems to be substantial, and it would be interesting to investigate the reasons behind this. The code for the interactive visualizations were based on modifications of this code: https://www.d3-graph-gallery.com/graph/barplot_button_data_hard.html "],["conclusion.html", "Chapter 7 Conclusion", " Chapter 7 Conclusion The main limitations of this project–related to the fact that we cannot distinguish between administrative errors and lies–were discussed at the outset. Despite this challenge, it is possible, at least for variables that I examined, to identify a few patterns that predict over-reporting. It would be interesting, time permitting, to look at additional political variables, such as margins of the elections, which party won, etc. to see if these electoral outcomes also have an effect on over-reporting. The issue here would be that there electoral outcome would be identical for all those within the administrative unit relevant for the outcome i.e. state, congressional district (CD), so ideally for this analysis we would have additional years of data to allow for state/CD level variation. "]]
