[["index.html", "Self-reported voter turnout Chapter 1 Introduction", " Self-reported voter turnout Jiyeon Chang 2020-12-18 Chapter 1 Introduction Average turnout in US elections hover around 50-60%, but in surveys, the figure is often closer to 70-80%. Why is that? It’s possible that this gap is attributable to nonresponse, but some point out that there is an element of social desirability bias in the way people respond to survey questions. In other words, there are people who lie about whether they voted, i.e. they say they voted even though in reality they did not. This has been a tricky issue for survey researchers, as you can’t take survey response at face value. In this project, I look at what are some of the variables that can may explain over-reporting pattern. The main dataset I use to explore this question is the Cooperative Congressional Election Study (https://cces.gov.harvard.edu/). The survey asks respondents whether they voted in the recent election. For each respondent, there is a variable indicating whether there exists a record of that person of having voted. So the strategy here is to look at respondents for whom there is no record of voting, and compare the group who said they voted (record does not match what they say) vs. those who say that they did not vote (missing record matches their statement). For those who claim to have voted, there are two possible explanations for why there might not be a corresponding voting record for them. First, it may have been an administrative error–if their address has changed recently, or some other identifying information does not match, it’s possible that the missing record is not an indication that they are lying. Alternatively, the respondent may not be telling the truth. There is not definitive way to distinguish which of the observations are due to error or due to lying. However, if we can assume that the rate of administrative error should be random across different populations, then it is possible to see how share of those who claim to have voted (vs. not) differ across individuals with specific demographic, political or social traits. Specifically, I report the result of looking at the following variables: state (proxies political climate) age; income; race; political affiliation; strength of partisanship (demographic variables) how long Rs have lived at the current address; frequency of church attendance (ties to community) by election cycle; general election including presidential vs. off yr etc. (election specific variables) turnout; margin of elections (This data will come from the MIT Elections Data) (if time allows) "],["data-sources.html", "Chapter 2 Data sources", " Chapter 2 Data sources The main dataset I use is the Cooperative Congressional Election Study (https://cces.gov.harvard.edu/). CCES is a 50,000+ person national stratified sample survey administered by YouGov on a biannual basis. For the purpose of this project, I look at the 3 most recent presidential elections for which data is available–that is, 2008, 2012 and 2016. The data from 2016, for instance, includes 64,600 observations and 563 variables. However, I look a subset of this sample, namely those for whom there is no voting record. For the 3 years, this reduces the sample to a total of 13,216 observations. A wide range of variables are included in the dataset, including the main demographic variables (age, gender, state/county of residence, race, education, income, etc.) and an extensive list of questions about political affiliation and attitudes, support for specific candidates and opinions about proposed policies, such as climate change, and gun control. As briefly mentioned above, one of the challenges of using this data is that, for those who claim to have voted (but for whom there is no matching voting record), we won’t ever know whether these people are lying or if there’s been an administrative error. Literature pertaining to the field identifies a number of factors that may predict a higher probability of admin error, e.g. recent change in residence. There are many variables for which the rate of administrate error should not differ across the response categories, e.g. gender, or religion. This is more a problem with the conceptualization of the question rather than a problem with the quality of data, however, and I will address the limitations in the conclusions one can draw from the visualization that I will produce. "],["data-transformation.html", "Chapter 3 Data transformation", " Chapter 3 Data transformation The data has been prepared by merging data from 3 election years, and also merging the CCES data with electoral outcome dataset to create variables accounting for state level turnout and margin of victory. It takes a while to load the data, so I display the code below but don’t run it, and the data used for the next section calls it from a csv file I saved in earlier runs. The main steps I undertook involved the following: Rename variables from each year to prepare for the merger. A similar set of questions were asked each year but they didn’t necessarily have the same variable names. Recode variables with several levels into binary variables where it is helpful: This is done for variables such as church01, which is 1 if the person goes to church at least several times a year and 0 otherwise. (The original variable had more detailed frequencies.) Similar steps were taken to indicate stable_residency01, which is 1 if the person has been residing at current address for &gt;1 year. The electoral outcome at state-year level from the MIT Elections data was merged with the respondent’s state-year to create variables indicating how big the margin was in the race they voted, and which party won. These variables make it possible to capture the “political reality” of the electoral unit (state) in which each respondent is voting. Whether this political reality predicts differential rate of overreporting corresponds to item (e) on my list of questions; as mentioned above I’m not sure if I’ll have time to get to it but the variables have been merged and created! "],["missing-values.html", "Chapter 4 Missing values", " Chapter 4 Missing values My dataset is quite large, so for the purpose of visualization, I restrict the analysis to 2016, and assume that the pattern will be similar for years 2012 and 2008. Moreover, I deleted the variables which were derived from other variables, as their missing pattern would be exactly the same. I also deleted variables whose missingness is not meaningful (e.g. variable for additional notes, which is (a) optional and (b) text data). ## turnout_self01 residence pid7 newsint pew_churatd ## 3580 33 25 20 11 ## faminc state tookpost gender race ## 6 0 0 0 0 ## age educ vv_turnout_gvm vv_regstatus ## 0 0 0 0 ## nothing turnout_self01 ## 17202 3490 ## pew_churatd residence ## 204 27 ## newsint faminc ## 13 4 ## pid7 newsint, turnout_self01 ## 20 7 ## pew_churatd, turnout_self01 residence, pew_churatd ## 77 2 ## pid7, residence pid7, pew_churatd ## 2 1 ## faminc, turnout_self01 residence, turnout_self01 ## 2 2 ## pid7, turnout_self01 ## 2 As one can see immediately from the charts, the only variable with a substantial number of missing values is the turnout_self01, i.e. self-reported turnout, which is the dependent variable of interest. Out of the 5,783 observations, 916 are missing this value. The variable that has the next highest number of values is pid7 i.e. political affiliation using 7 scales, with only 9 missing values. There are 4 additional variables with missing values (newint, residence, faminc) but they are in the single digits and given that these constitute a neglibile share of the sample, it would be easiest to drop these values from the analysis. As for the missing turnout variable, given that this is the dependent variable of interest, I wouldn’t feel comfortable imputing the value. As a result, I will drop the NAs by default in all illustrations unless otherwise indicated. "],["results.html", "Chapter 5 Results 5.1 Are there state-level variations in over-reporting? 5.2 Demographic and political traits of respondents 5.3 Does the size of our social network predict over-reporting?", " Chapter 5 Results ## ## 0 1 ## 9118 27286 ## # A tibble: 6 x 5 ## # Groups: state, year [6] ## state year turnout_self01 n freq ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Alabama 2012 NA 29 0.125 ## 2 Alabama 2016 NA 49 0.168 ## 3 Alaska 2012 NA 2 0.08 ## 4 Alaska 2016 NA 1 0.0323 ## 5 Arizona 2012 NA 55 0.180 ## 6 Arizona 2016 NA 94 0.202 5.1 Are there state-level variations in over-reporting? For year 2016, it seems that over-reporting rates tended to be higher in coastal states, and relatively lower in the inner states. The pattern is not repeated in previous years, however, as the earlier maps indicate. The cleveland plot of overrporting rates allows us to see the range of over-reporting across states for each year, as well as to identify trends across years. While the share of the sample claiming to have voted does not form a tight band around the average at state-level, we can see that there is a pattern over time; over-reporting rates were much lower in 2008, whereas the blue dots lying to the right of green suggest that the values for 2016 in generally tended to be higher than in 2012. This plot shows that the share of respondents with missing values for the self-reported turntout (NA) shows a correlation with state. The green and blue dot roughly suggest that a state with a higher overreporting rate in 2012 also tended to have higher rate in 2016. Notice also the very low NA values for 2008. Many of the states also did not have a single NA, which is indicated by the missing red dot for at state level. This likely suggests that there was a change in the way the survey was conducted. It might not have been possible to proceed with the survey without answering the question in 2008, but that this practice changed in subsequent years. The map and the cleveland plots combined suggest that the state-level information aren’t significant predictors of over-reporting. In any case, any discrepncies that are observed across states are likely to be attributable to administrative errors, rather than “cultural” variation in the likelihood that an individual would feel pressured to lie about not having voted. On that note, next we look at some of the demographic and political traits of individuals that may explain over-reporting. 5.2 Demographic and political traits of respondents Those with higher level of education are more likely than others to over-report turnout. While it’s possible that this population also votes at a higher rate, the underlying data only look at those for whom there is no record of having voted, and the mosaic plot indicates that this pattern is found commonly in female as well as male respondents. We can also deduce that at each level of income, males over-report at slightly higher rate than do women, but that the difference across gender categories are smaller than the differences across educational levels. overreporting by strong partisanship and party affiliation Now we look at how overreporting rate differs by party affiliation, and It shows that the overall pattern of over-reporting is similar for both female and male, but that on average, men are marginally more likely to overreport, regardless of their political affiliation. What is especially interesting is the fact that Independents are less likely than both Democrats and Republicans to overreport, and that overrepoting rate is especially low for those who responded that they were “Unsure” about their political affiliation. In other words, the stronger the political affiliation/identity, the more likely an individual is to say that they voted when in fact they did not. There may be a number of reasons for this behavior. Those who have strong interest in politics are likely to recognize the importance of voting, and conditional on not having voted, the guilt of not having done so and the embarrassment of others finding out would weigh in heavier on this population than others for whom it doesn’t matter as much. 5.3 Does the size of our social network predict over-reporting? Similar to the demographic variables surveyed above, one would wonder, does having a large/strong social network influence how likely you are to over-report? While there are no perfect measures of social network, two variables included in the CCES stand out: (1) length of residency at current address and (2) frequency of church attendance. There are caveats that we need to take into consideration of course. If I moved to a new address, but that new location is wiithin my old neighborhood, having only lived there for a month doesn’t mean I have weak ties to my neighbors. Similarly, many people don’t there are people who don’t have any religious social network, but are nonetheless are very tightly embedded in their local social network. To deal with the second potential criticism, I limit the extrapolation I do on the church variable to “church-goers.” In other words, whatever conclusion I draw shall be limited to those who would consider going to church at any point vs. those who are not Christians, or would never consider going to church. ## ## 1 to 2 years 1 to 6 months 2 to 6 months 3 to 4 years ## 6262 891 2751 5942 ## 5 or more years 7 to 11 months Less than 1 month ## 23712 2151 524 ## ## Less than 1 month 1 to 6 months 7 to 11 months 1 to 2 years ## 524 891 2151 6262 ## 3 to 4 years 5 or more years ## 5942 23712 ## [1] &quot;Less than 1 month&quot; &quot;1 to 6 months&quot; &quot;7 to 11 months&quot; ## [4] &quot;1 to 2 years&quot; &quot;3 to 4 years&quot; &quot;5 or more years&quot; ## ## A few times a year Don&#39;t know More than once a week ## 6424 534 3448 ## Never Once a week Once or twice a month ## 11103 7063 3509 ## Seldom ## 10177 ## [1] &quot;More than once a week&quot; &quot;Once a week&quot; &quot;Once or twice a month&quot; ## [4] &quot;A few times a year&quot; &quot;Seldom&quot; &quot;Never&quot; ## [7] &quot;Don&#39;t know&quot; The above plot shows that indeed those who are frequent church goes are also more religious. However, even for those who go to church only seldom, more than half of the respondents from this group say that religion is very or somewhat important. FYI: I wanted to have the “Very important” appear on the top of the legend, but I couldn’t figure out a way to do it without changing the factor levels at the data frame level, which would also reverse the presentation of the chart, which is not what I wanted. So current listing is not ideal, but I kep it as is. by election cycle; general election including presidential vs. off yr etc. (election specific variables) turnout; margin of elections (This data will come from the MIT Elections Data) (if time allows) "],["interactive-component.html", "Chapter 6 Interactive component", " Chapter 6 Interactive component text new "],["conclusion.html", "Chapter 7 Conclusion", " Chapter 7 Conclusion The main limitations of this project–related to the fact that we cannot distinguish between administrative errors and lies–were discussed at the outset. Despite this challenge, it is possible, at least for variables that we Discuss limitations and future directions, lessons learned. "]]
